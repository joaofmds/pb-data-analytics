- Armazenamento e Processamento Paralelo

	- Introdução

	- O Que é um Cluster de Computadores?
		- Um servidor é um computador que serve serviços de armazenamento, aplicações ou banco de dados
		- Um servidor possui escalabilidade vertical, há um limite até onde conseguimos incluir mais espaço em disco, mais processadores e mais memória RAM
		- Um cluster de computadores é um conjunto de servidores com um mesmo propósito visando fornecer um tipo de serviço, como armazenamento ou processamento de dados
		- Um cluster possui escalabilidade horizontal, se quisermos aumentar a capacidade computacional incluímos mas máquinas no cluster

	- O Que é Armazenamento Paralelo?
		- Com cluster aumentamos a capacidade computacional
		- O armazenamento paralelo consiste em distribuir o armazenamento de dados através de diversos servidores

	- Software para Armazenamento Paralelo - Apache Hadoop - Parte 1/2
		- Como gerenciamos o armazenamento paralelo através de diversos computadores?
		- Precisamos de um sistema de arquivos distribuídos
		- Os PCs tem um sistema de arquivos (NTFS, ext4 etc.) mas ele não foi desenvolvido para armazenamento distribuído
		- Apache Hadoop HDFS

	- Software para Armazenamento Paralelo - Apache Hadoop - Parte 2/2
		- O HDFS é o software responsável pela gestão do cluster de computadores definindo como os arquivos serão distribuídos através do cluster
		- Com o HDFS podemos construir um Data Lake que rode sobre um cluster de computadores e permite o armazenamento de grandes volumes de dados com hardware commodity
		- Isso permitiu que o Big Data pudesse ser usado em larga escala

	- Processamento Paralelo de Big Data
		- Como processar os dados se eles estão distribuídos em diversos computadores?
		- No processamento paralelo o objetivo é dividr uma tarefa em várias sub-tarefas e executá-los em paralelo
		- O Apache Hadoop MapReduce e o Apache Spark são dois frameworks para este propósito
		- Ao usar um framework de processamento paralelo, as sub-tarefas são levadas para o processador da máquina do cluster onde os dados estão armazenados, aumentando assim a velocidade de processamento de grandes volumes de dados

	- Arquitetura de Armazenamento e Processamento Paralelo
		- Apache Hadoop
			- O HDFS é um serviço rodando em todas as máquinas do cluster, sendo um NameNode para gerenciar o cluster e os DataNodes que fazem o trabalho de armazenamento propriamente dito
			- O MapReduce também é um serviço rodando em todas as máquinas do cluster, sendo um Job Tracker para gerenciar o processamento e os Task Trackers que fazem o trabalho do processamento
			- O Job Tracker consulta o NameNode a fim de saber a localização dos blocos de dados nas máquinas do cluster
			- Os Task Trackers se comunicam com os DataNodes para obter os dados do disco, executar o processamento e então retornar o resultado ao Job Tracker
			- Essa arquitetura permite armazenar e processar grandes quantidades de dados e assim extrair valor do Big Data através da análise de dados 
			- MapReduce Layer
			- HDFS Layer
				- Master Node
				- Worker 
	
    - Soluções de Armazenamento e Processamento Paralelo - Parte 1/2
		- Apache Hadoop
		- Cloudera
	
    - Soluções de Armazenamento e Processamento Paralelo - Parte 2/2
		- Azure HDInsight
		- Amazon EMR
		- Databricks